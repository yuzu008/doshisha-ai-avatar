import streamlit as st
import os
from langchain.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
# from openai import OpenAI # ã“ã®è¡Œã¯ä¸è¦ã«ãªã‚‹ã®ã§ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã¾ãŸã¯å‰Šé™¤

# --- APIã‚­ãƒ¼ã®è¨­å®š ---
os.environ["OPENAI_API_KEY"] = "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"
# --- ã“ã“ã¾ã§ ---

# --- è¨­å®šé …ç›® ---
DB_LOAD_PATH = "doshisha_faiss_db"
EMBEDDING_MODEL = "intfloat/multilingual-e5-large"

# --- ã‚¢ãƒ—ãƒªã®åˆæœŸè¨­å®š ---
st.set_page_config(page_title="åŒå¿—ç¤¾å¤§å­¦AIã‚¢ãƒã‚¿ãƒ¼", page_icon="ğŸ¤–", layout="wide")

# --- é–¢æ•°ã®å®šç¾© ---
@st.cache_resource
def load_dependencies():
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    db = FAISS.load_local(DB_LOAD_PATH, embeddings, allow_dangerous_deserialization=True)
    retriever = db.as_retriever(search_kwargs={"k": 5})
    return retriever

# --- Webã‚¢ãƒ—ãƒªã®ç”»é¢æç”» ---
st.title("ğŸ¤– åŒå¿—ç¤¾å¤§å­¦AIã‚¢ãƒã‚¿ãƒ¼")
st.caption("åŒå¿—ç¤¾å¤§å­¦å…¬å¼ã‚µã‚¤ãƒˆã®æƒ…å ±ã‚’åŸºã«ã€AIãŒè³ªå•ã«å¯¾è©±å½¢å¼ã§å›ç­”ã—ã¾ã™ã€‚")

if not ("OPENAI_API_KEY" in os.environ and os.environ["OPENAI_API_KEY"].startswith("sk-")):
    st.error("OpenAI APIã‚­ãƒ¼ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚³ãƒ¼ãƒ‰å†…ã®è©²å½“ç®‡æ‰€ã‚’ã‚‚ã†ä¸€åº¦ã”ç¢ºèªãã ã•ã„ã€‚")
else:
    try:
        # ä¾å­˜é–¢ä¿‚ï¼ˆDBã¨Retrieverï¼‰ã‚’ãƒ­ãƒ¼ãƒ‰
        retriever = load_dependencies()

        # --- ã€æœ€é‡è¦ä¿®æ­£ç‚¹ã€‘LLMã®åˆæœŸåŒ–æ–¹æ³•ã‚’LangChainã®æ¨™æº–çš„ãªæ–¹æ³•ã«æˆ»ã—ã¾ã™ ---
        # ã“ã‚ŒãŒæœ€æ–°ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªé–“ã®é€£æºã§æœ€ã‚‚å®‰å®šã—ãŸæ›¸ãæ–¹ã§ã™
        llm = ChatOpenAI(model_name="gpt-4o", temperature=0)
        # --- ã“ã“ã¾ã§ä¿®æ­£ ---

        prompt = PromptTemplate(
            template="""ã‚ãªãŸã¯åŒå¿—ç¤¾å¤§å­¦ã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã‚‹ã€è¦ªåˆ‡ã§å„ªç§€ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        æä¾›ã•ã‚ŒãŸä»¥ä¸‹ã®ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€ã ã‘ã‚’åŸºã«ã—ã¦ã€è³ªå•ã«æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã«ç­”ãˆãŒãªã„å ´åˆã¯ã€ã€Œç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãã®æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚è‡ªèº«ã®çŸ¥è­˜ã§ç­”ãˆã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚

        ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã€‘
        {context}

        ã€è³ªå•ã€‘
        {input}

        ã€å›ç­”ã€‘
        """,
            input_variables=["context", "input"],
        )

        document_chain = create_stuff_documents_chain(llm, prompt)
        retrieval_chain = create_retrieval_chain(retriever, document_chain)

        question = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šå»ºå­¦ã®ç²¾ç¥ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ï¼‰", key="user_question")

        if question:
            st.write(f"**è³ªå•:** {question}")
            
            with st.spinner("AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­ã§ã™..."):
                response = retrieval_chain.invoke({"input": question})
                
                st.subheader("âœ… å›ç­”")
                st.write(response["answer"])

                st.subheader("--- å›ç­”ã®æ ¹æ‹ ã¨ãªã£ãŸæƒ…å ± ---")
                for i, doc in enumerate(response["context"]):
                    source_url = doc.metadata.get('source', 'ä¸æ˜ãªã‚½ãƒ¼ã‚¹')
                    with st.expander(f"ã€æ ¹æ‹ {i+1}ã€‘ {doc.page_content[:50]}..."):
                        st.write(doc.page_content)
                        st.link_button("å‡ºå…¸å…ƒãƒšãƒ¼ã‚¸ã‚’é–‹ã", url=source_url)

    except Exception as e:
        st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")